"""
Test script to verify the new optimized architecture works correctly.

This script will:
1. Simulate crawling and inserting some news items
2. Run the prioritizer to score and queue items
3. Run the summarizer worker to process items
"""

import asyncio
import os
import sys
from datetime import datetime, timezone
from unittest.mock import AsyncMock, patch
import httpx

# Add the backend directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '.'))

from app.crawler.spider import default_sources
from app.db.database import get_supabase_client, insert
from app.prioritizer import prioritize_news
from app.workers.summarizer_worker import SummarizerWorker


async def test_new_architecture():
    print("Testing the new optimized architecture...\n")
    
    # Test 1: Insert some test news items directly into the database
    print("1. Inserting test news items into database...")
    supabase = await get_supabase_client()
    
    # Clear any existing test data
    try:
        supabase.table("news").delete().ilike("title", "Test News%").execute()
    except:
        pass  # Ignore if table doesn't exist yet
    
    # Insert test news items with NULL summaries (as the crawler would do in the new system)
    test_items = [
        {
            "title": "Test News 1: Government Announces New Policy",
            "url": "https://example.com/news1",
            "summary": None,  # This should remain NULL until processed by summarizer
            "source": "kompas.com",
            "category": "politics",
            "publish_date": datetime.now(timezone.utc).isoformat(),
            "crawl_date": datetime.now(timezone.utc).isoformat(),
            "content_hash": "testhash1"
        },
        {
            "title": "Test News 2: Technology Innovation in Indonesia",
            "url": "https://example.com/news2", 
            "summary": None,  # This should remain NULL until processed by summarizer
            "source": "detik.com",
            "category": "technology",
            "publish_date": datetime.now(timezone.utc).isoformat(),
            "crawl_date": datetime.now(timezone.utc).isoformat(),
            "content_hash": "testhash2"
        },
        {
            "title": "Test News 3: Economy Update",
            "url": "https://example.com/news3",
            "summary": None,  # This should remain NULL until processed by summarizer
            "source": "cnbcindonesia.com",
            "category": "economy", 
            "publish_date": datetime.now(timezone.utc).isoformat(),
            "crawl_date": datetime.now(timezone.utc).isoformat(),
            "content_hash": "testhash3"
        }
    ]
    
    inserted_ids = []
    for item in test_items:
        result = supabase.table("news").insert(item).execute()
        if result.data:
            inserted_ids.append(result.data[0]['id'])
    
    print(f"   Inserted {len(inserted_ids)} test news items with NULL summaries")
    
    # Verify that summaries are NULL
    for item_id in inserted_ids:
        result = supabase.table("news").select("summary").eq("id", item_id).execute()
        if result.data and result.data[0]['summary'] is None:
            print(f"   ✓ News item {item_id} has NULL summary as expected")
        else:
            print(f"   ✗ News item {item_id} does not have NULL summary")
    
    # Test 2: Run the prioritizer
    print("\n2. Running prioritizer to score and queue items...")
    queued_count = await prioritize_news()
    print(f"   Prioritizer completed. Queued {queued_count} items for summarization")
    
    # Test 3: Mock the LLM API call to avoid actual API usage during testing
    print("\n3. Testing summarizer worker with mocked LLM API...")
    
    # Create a worker instance
    worker = SummarizerWorker(max_concurrent=1)
    await worker.initialize()
    
    # Mock the summarize_with_llm function to return a test summary
    with patch('app.utils.content_extractor.summarize_with_llm', new_callable=AsyncMock) as mock_summarize:
        mock_summarize.return_value = "This is a test summary generated by the LLM."
        
        # Also mock the content extraction to return test content
        with patch('app.utils.content_extractor.extract_article_content', new_callable=AsyncMock) as mock_extract:
            mock_extract.return_value = "This is the content of the news article that would normally be extracted from the URL. It contains several paragraphs of text that the LLM would summarize."
            
            # Process a single batch of tasks
            processed_count = await worker.run_once()
            print(f"   Summarizer worker processed {processed_count} items")
    
    # Verify that the summaries were updated in the database
    print("\n4. Verifying summaries were updated in database...")
    all_updated = True
    for item_id in inserted_ids:
        result = supabase.table("news").select("summary").eq("id", item_id).execute()
        if result.data and result.data[0]['summary'] is not None:
            print(f"   ✓ News item {item_id} summary updated: {result.data[0]['summary'][:50]}...")
        else:
            print(f"   ✗ News item {item_id} summary not updated")
            all_updated = False
    
    if all_updated:
        print("\n✓ All tests passed! The new architecture is working correctly.")
    else:
        print("\n✗ Some tests failed.")
    
    # Clean up test data
    try:
        supabase.table("news").delete().ilike("title", "Test News%").execute()
        print("   Cleaned up test data")
    except:
        print("   Could not clean up test data (might not exist)")
    
    return all_updated


if __name__ == "__main__":
    success = asyncio.run(test_new_architecture())
    sys.exit(0 if success else 1)